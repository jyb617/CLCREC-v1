#!/usr/bin/env python3
"""
è‡ªåŠ¨æ‰¾åˆ° RTX 4090 + 90GB RAM çš„æœ€ä¼˜è®­ç»ƒé…ç½®
"""

import torch
import psutil
import subprocess
import json
from pathlib import Path

def get_gpu_memory():
    """è·å–GPUæ˜¾å­˜ä¿¡æ¯ï¼ˆGBï¼‰"""
    if torch.cuda.is_available():
        props = torch.cuda.get_device_properties(0)
        total = props.total_memory / 1024**3
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        return total, allocated, reserved
    return 0, 0, 0

def get_cpu_memory():
    """è·å–CPUå†…å­˜ä¿¡æ¯ï¼ˆGBï¼‰"""
    mem = psutil.virtual_memory()
    total = mem.total / 1024**3
    available = mem.available / 1024**3
    used = mem.used / 1024**3
    return total, available, used

def estimate_batch_size_limit():
    """ä¼°ç®—æœ€å¤§batch_size"""
    print("=" * 80)
    print("GPU æ˜¾å­˜åˆ†æ")
    print("=" * 80)

    total_mem, _, _ = get_gpu_memory()
    print(f"æ€»æ˜¾å­˜: {total_mem:.2f} GB")

    # ä¼°ç®—æ¨¡å‹å’Œä¼˜åŒ–å™¨å ç”¨
    # CLCRec æ¨¡å‹ç›¸å¯¹è¾ƒå°ï¼Œå¤§çº¦ 1-2GB
    model_memory = 2.0
    print(f"æ¨¡å‹+ä¼˜åŒ–å™¨é¢„ä¼°: ~{model_memory:.1f} GB")

    # å¯ç”¨äºbatchçš„æ˜¾å­˜
    available_for_batch = total_mem - model_memory - 2.0  # ä¿ç•™2GB buffer
    print(f"å¯ç”¨äºbatch: ~{available_for_batch:.1f} GB")

    # æ¯ä¸ªæ ·æœ¬çš„æ˜¾å­˜ä¼°ç®—
    # user_tensor: (batch_size * 129,) * 4 bytes
    # item_tensor: (batch_size * 129,) * 4 bytes
    # embeddings: batch_size * 129 * 64 * 4 bytes
    # æ··åˆç²¾åº¦ä¼šå‡åŠ
    bytes_per_sample = (129 * 2 * 4 + 129 * 64 * 2) / 1024**3  # GB

    max_batch = int(available_for_batch / bytes_per_sample)

    print(f"\næ¯ä¸ªæ ·æœ¬ä¼°ç®—æ˜¾å­˜: {bytes_per_sample * 1024:.2f} MB")
    print(f"ç†è®ºæœ€å¤§batch_size: ~{max_batch}")

    # ä¿å®ˆä¼°è®¡ï¼Œå–80%
    safe_max = int(max_batch * 0.8)
    print(f"å®‰å…¨æœ€å¤§batch_size: ~{safe_max}")

    # å»ºè®®çš„æµ‹è¯•å€¼
    test_values = []
    for size in [256, 512, 1024, 2048, 4096]:
        if size <= safe_max:
            test_values.append(size)

    print(f"\nå»ºè®®æµ‹è¯•çš„batch_size: {test_values}")
    print()

    return test_values

def estimate_num_workers_limit():
    """ä¼°ç®—æœ€å¤§num_workers"""
    print("=" * 80)
    print("CPU å†…å­˜åˆ†æ")
    print("=" * 80)

    total_mem, available_mem, used_mem = get_cpu_memory()
    print(f"æ€»å†…å­˜: {total_mem:.2f} GB")
    print(f"å·²ä½¿ç”¨: {used_mem:.2f} GB")
    print(f"å¯ç”¨: {available_mem:.2f} GB")

    # è·å–CPUæ ¸å¿ƒæ•°
    cpu_count = psutil.cpu_count(logical=False)
    logical_count = psutil.cpu_count(logical=True)
    print(f"ç‰©ç†æ ¸å¿ƒ: {cpu_count}")
    print(f"é€»è¾‘æ ¸å¿ƒ: {logical_count}")

    # ä¼°ç®—æ¯ä¸ªworkerçš„å†…å­˜å ç”¨
    # ä¸»è¦æ˜¯æ•°æ®åŠ è½½ï¼Œmovielensæ•°æ®é›†è¾ƒå°
    # æ¯ä¸ªworkerå¤§çº¦éœ€è¦ 0.5-1GB
    memory_per_worker = 1.0  # GB

    # åŸºäºå†…å­˜çš„æœ€å¤§workers
    max_workers_by_memory = int((available_mem - 10) / memory_per_worker)  # ä¿ç•™10GB

    # åŸºäºCPUçš„æ¨èworkers
    # ä¸€èˆ¬å»ºè®®ï¼š4-8ä¸ªworkersï¼Œä¸è¶…è¿‡ç‰©ç†æ ¸å¿ƒæ•°
    max_workers_by_cpu = min(cpu_count, 16)

    max_workers = min(max_workers_by_memory, max_workers_by_cpu)

    print(f"\næ¯ä¸ªworkerä¼°ç®—å†…å­˜: ~{memory_per_worker:.1f} GB")
    print(f"åŸºäºå†…å­˜çš„æœ€å¤§workers: {max_workers_by_memory}")
    print(f"åŸºäºCPUçš„æ¨èworkers: {max_workers_by_cpu}")
    print(f"æ¨èæœ€å¤§workers: {max_workers}")

    # å»ºè®®çš„æµ‹è¯•å€¼
    test_values = []
    for size in [4, 8, 12, 16]:
        if size <= max_workers:
            test_values.append(size)

    print(f"\nå»ºè®®æµ‹è¯•çš„num_workers: {test_values}")
    print()

    return test_values

def estimate_optimal_config():
    """ç»¼åˆä¼°ç®—æœ€ä¼˜é…ç½®"""
    print("\n" + "â–ˆ" * 80)
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" + " " * 20 + "RTX 4090 æœ€ä¼˜é…ç½®åˆ†æ" + " " * 21 + "â–ˆ")
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" * 80)
    print()

    batch_sizes = estimate_batch_size_limit()
    num_workers_list = estimate_num_workers_limit()

    print("=" * 80)
    print("æ¨èé…ç½®")
    print("=" * 80)

    # æ¨èé…ç½®
    if batch_sizes:
        recommended_batch = batch_sizes[-1] if len(batch_sizes) > 0 else 512
        if recommended_batch > 1024:
            recommended_batch = 1024  # å¤ªå¤§çš„batchå¯èƒ½å½±å“æ”¶æ•›
    else:
        recommended_batch = 512

    if num_workers_list:
        recommended_workers = num_workers_list[-1] if len(num_workers_list) > 0 else 8
    else:
        recommended_workers = 8

    print(f"\nğŸ¯ æ¨èé…ç½®ï¼š")
    print(f"   --batch_size={recommended_batch}")
    print(f"   --num_workers={recommended_workers}")

    print(f"\nğŸ“Š å®Œæ•´å‘½ä»¤ï¼š")
    print(f"""
python main.py \\
  --batch_size={recommended_batch} \\
  --num_workers={recommended_workers} \\
  --l_r=0.001 \\
  --reg_weight=0.1 \\
  --num_neg=128 \\
  --has_a=True \\
  --has_t=True \\
  --has_v=True \\
  --lr_lambda=0.5 \\
  --temp_value=2.0 \\
  --num_sample=0.5
""")

    print("\n" + "=" * 80)
    print("é…ç½®ç»„åˆæµ‹è¯•å»ºè®®")
    print("=" * 80)

    print("\nä»ä¿å®ˆåˆ°æ¿€è¿›çš„æµ‹è¯•æ–¹æ¡ˆï¼š")

    configs = [
        {"batch": 256, "workers": 4, "level": "ä¿å®ˆï¼ˆå½“å‰ï¼‰"},
        {"batch": 512, "workers": 8, "level": "æ¨è"},
        {"batch": 1024, "workers": 8, "level": "æ¿€è¿›"},
        {"batch": 1024, "workers": 12, "level": "æé™"}
    ]

    for i, cfg in enumerate(configs, 1):
        if cfg["batch"] in batch_sizes and cfg["workers"] in num_workers_list:
            print(f"\n{i}. {cfg['level']}é…ç½®:")
            print(f"   python main.py --batch_size={cfg['batch']} --num_workers={cfg['workers']} ...")

    print("\n" + "=" * 80)
    print("æ€§èƒ½é¢„ä¼°")
    print("=" * 80)

    print(f"""
å½“å‰é€Ÿåº¦: ~3400 it/s (batch_size=256, num_workers=4)

é¢„æœŸé€Ÿåº¦ï¼š
- batch_size=512, num_workers=8:  ~6000-7000 it/s  (1.8-2.0x)
- batch_size=1024, num_workers=8: ~8000-10000 it/s (2.4-3.0x)
- batch_size=1024, num_workers=12: ~10000-12000 it/s (3.0-3.5x)
""")

    print("\n" + "=" * 80)
    print("æ³¨æ„äº‹é¡¹")
    print("=" * 80)
    print("""
1. ä»æ¨èé…ç½®å¼€å§‹æµ‹è¯•ï¼Œå¦‚æœæ²¡æœ‰OOMï¼Œå¯ä»¥å°è¯•æ›´å¤§çš„batch
2. batch_sizeè¿‡å¤§å¯èƒ½å½±å“æ¨¡å‹æ”¶æ•›ï¼Œæ³¨æ„è§‚å¯Ÿloss
3. num_workersè¿‡å¤šä¼šå¢åŠ CPUå’Œå†…å­˜å‹åŠ›ï¼Œè§‚å¯Ÿç³»ç»Ÿè´Ÿè½½
4. å¦‚æœé‡åˆ°OOMï¼š
   - é™ä½batch_size
   - å‡å°‘num_neg (ä»128é™åˆ°64)
   - æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨æ˜¾å­˜
5. ç›‘æ§GPUåˆ©ç”¨ç‡åº”è¯¥åœ¨90-100%ï¼Œå¦‚æœä½äº80%ï¼Œå¢åŠ batch_size
""")

    return recommended_batch, recommended_workers

def create_quick_test_script(batch_size, num_workers):
    """åˆ›å»ºå¿«é€Ÿæµ‹è¯•è„šæœ¬"""
    script_path = Path("quick_test.sh")

    content = f"""#!/bin/bash
# å¿«é€Ÿæµ‹è¯•è„šæœ¬ - åªè·‘1ä¸ªepochæ¥æµ‹è¯•é€Ÿåº¦

echo "=========================================="
echo "å¿«é€Ÿæ€§èƒ½æµ‹è¯•"
echo "batch_size={batch_size}, num_workers={num_workers}"
echo "=========================================="

python main.py \\
  --batch_size={batch_size} \\
  --num_workers={num_workers} \\
  --num_epoch=1 \\
  --l_r=0.001 \\
  --reg_weight=0.1 \\
  --num_neg=128 \\
  --has_a=True \\
  --has_t=True \\
  --has_v=True \\
  --lr_lambda=0.5 \\
  --temp_value=2.0 \\
  --num_sample=0.5

echo ""
echo "æµ‹è¯•å®Œæˆï¼æ£€æŸ¥ä¸Šé¢çš„ it/s é€Ÿåº¦"
"""

    script_path.write_text(content)
    script_path.chmod(0o755)

    print(f"\nâœ“ å·²åˆ›å»ºå¿«é€Ÿæµ‹è¯•è„šæœ¬: {script_path}")
    print(f"  è¿è¡Œ: ./quick_test.sh")

if __name__ == "__main__":
    try:
        batch_size, num_workers = estimate_optimal_config()
        create_quick_test_script(batch_size, num_workers)

        print("\n" + "=" * 80)
        print("ä¸‹ä¸€æ­¥")
        print("=" * 80)
        print("""
1. å…ˆç­‰å½“å‰è®­ç»ƒepochç»“æŸï¼ˆæˆ–æŒ‰ Ctrl+C åœæ­¢ï¼‰
2. è¿è¡Œå¿«é€Ÿæµ‹è¯•: ./quick_test.sh
3. è§‚å¯Ÿé€Ÿåº¦æå‡ï¼Œå¦‚æœæ»¡æ„ï¼Œä½¿ç”¨æ¨èé…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ
4. å¦‚æœæƒ³è¦æ›´æ¿€è¿›çš„é…ç½®ï¼Œæ‰‹åŠ¨ä¿®æ”¹ quick_test.sh ä¸­çš„å‚æ•°
""")

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
